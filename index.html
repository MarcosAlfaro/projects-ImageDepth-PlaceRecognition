<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Visual Place Recognition Using Omnidirectional Cameras and Monocular Depth Estimation">
  <meta property="og:title" content="Visual Place Recognition Using Omnidirectional Cameras and Monocular Depth Estimation"/>
  <meta property="og:description" content="This paper presents CrossPlace, an innovative method for cross-modal place recognition between omnidirectional cameras and LiDAR that transforms both data types into a common space of intensity, depth and semantics, enabling the use of a single network architecture for both modalities. The method includes data transformation and preprocessing with vertical interpolation and inpainting techniques, late fusion via descriptor concatenation, and achieves state-of-the-art performance on KITTI-360 dataset."/>
  <meta property="og:url" content="https://MarcosAlfaro.github.io/projects-ImageDepthPR/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/Fig1.PNG" />
  <meta property="og:image:width" content="2766"/>
  <meta property="og:image:height" content="770"/>


  <!-- <meta name="twitter:title" content="VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition">
  <meta name="twitter:description" content="We propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a self-supervised manner and brings them into a shared feature space. We achieve state-of-the-art performance in cross-modal retrieval on the Oxford RobotCar, ViViD++ datasets and KITTI benchmark, while maintaining high uni-modal global localization accuracy."> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/teaser_figure_w_traj.jpg">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
 <meta name="keywords" content="place recognition, panoramic images, monocular depth estimation, data fusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Visual Place Recognition Using Omnidirectional Cameras and Monocular Depth Estimation</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Visual Place Recognition Using Omnidirectional Cameras and Monocular Depth Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://arvc.umh.es/personal/malfaro/index.php?type=per&dest=publi&lang=es&vista=normal&idp=malfaro" target="_blank">Marcos Alfaro</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://arvc.umh.es/personal/jcabrera/index.php?type=per&dest=publi&lang=en&vista=normal&idp=jcabrera" target=_blank>Juan José Cabrera</a><sup>1</sup>,</span>
                  <span class="author-block">
                  <a href="https://arvc.umh.es/personal/arturo/index.php?type=per&dest=inicio&lang=en&vista=normal&idp=arturo&ficha=on" target=_blank>Arturo Gil</a><sup>1</sup>,</span>
                  <span class="author-block">
                  <a href="https://arvc.umh.es/personal/oscar/index.php?type=per&dest=inicio&lang=es&vista=normal&idp=oscar&ficha=on" target=_blank>Oscar Reinoso</a><sup>1,2</sup>,</span>
                    <span class="author-block">
                      <a href="https://arvc.umh.es/personal/lpaya/index.php?type=per&dest=inicio&lang=en&vista=normal&idp=lpaya&ficha=on" target="_blank">Luis Payá</a><sup>1,2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Institute for Engineering Research (I3E), Miguel Hernandez University of Elche (Spain) </span> <br>
                    <span class="author-block"><sup>2</sup>Valencian Graduate School and Research Network for Artificial Intelligence (valgrAI) </span> <br>

                    <!-- <span class="author-block">Technical University of Munich<br>arXiv 2024</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>
                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>2</sup>Munich Center for Machine Learning</span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>3</sup>Microsoft</span>
                  </div> -->
                  
                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">3DV 2025</span>
                  </div> -->
                  

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                         <div class="item">
                          <!-- Your image here -->
                          <!-- <img src="static/images/3dv2025_banner.png" alt="3DV2025" class="center" width="50%"/> -->
                         </div>
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/2403.14594.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark" disabled="">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary (Coming Soon)</span>
                    </a>
                  </span> -->
                  
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MarcosAlfaro/DepthCosPlace" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2403.07593" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
  <div class="hero-body">
  <div class="grid-container">
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop>
        <source src="static/compressed/2015-02-13-09-16-26.mp4"
        type="video/mp4">
        </video>
    </div>
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop>
        <source src="static/compressed/business_run3.mp4"
        type="video/mp4">
      </video>
    </div>
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop>
        <source src="static/compressed/residential_run3.mp4"
        type="video/mp4">
      </video>
    </div>  
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop>
        <source src="static/compressed/university_run3.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</div>
</div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="static/images/Fig1.PNG" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        General outline of the proposed method. First, relative depth maps are obtained from panoramic images using Depth Anything v2 [1]. Second, both images and depth maps are transformed into embeddings through the same frozen VPR model. Third, visual and depth embeddings are merged into a global descriptor through late fusion.
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Omnidirectional cameras are a suitable, cost-effective choice for Visual Place Recognition (VPR), as they provide comprehensive information from the scene regardless of the robot orientation. However, vision sensors are vulnerable to environmental appearance changes (e.g., illumination, season). While multi-modal approaches can overcome these challenges, they introduce significant cost and system complexity. This paper introduces a novel fusion framework that enhances VPR robustness by integrating visual data with geometric features derived from monocular depth estimation, retaining a single-camera setup. In the ablation study, both early and late fusion strategies are evaluated to optimally combine appearance-based and depth-derived descriptors. The extensive evaluation on challenging, indoor and outdoor datasets demonstrates that the proposed method consistently boosts retrieval performance across multiple state-of-the-art VPR backbones. Furthermore, this improvement is achieved without requiring end-to-end retraining, allowing our method to function as a pluggable module for pre-trained models. Consequently, this work presents a powerful, practical, and low-cost solution for robust VPR, with high potential to scale as monocular depth estimation and VPR models continue to improve.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Depth estimation and processing -->
<section class="hero teaser">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Depth Estimation and Processing</h2>
      <h2 class="subtitle">
        COLD database [2]
      </h2>
      <div class="columns is-centered">
        <div class="column">
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="cold_depth_videos.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
            We propose a novel fusion framework that enhances VPR robustness by integrating visual data with depth maps, which are preprocessed to adapt them as suitable inputs for VPR models.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="subtitle">
        360Loc database [3]
      </h2>
      <div class="columns is-centered">
        <div class="column">
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="360loc_depth_videos.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
            Depth maps obtained by means of Depth Anything v2 [1], a state-of-the-art depth estimation model, and preprocessed through various techniques to ensure compatibility with the VPR model knowledge and to enhance the geometric information captured.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End depth estimation and processing -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero teaser">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Trajectory Results</h2>
      <h2 class="subtitle">
        COLD database [2]
      </h2>
      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            FR-A Environment - Query Cloudy / Database Cloudy
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="FR_A_Cloudy.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            FR-A Environment - Query Night / Database Cloudy
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="FR_A_Night.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            SA-A Environment - Query Cloudy / Database Cloudy
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="SA_A_Cloudy.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            SA-B Environment - Query Sunny / Database Cloudy
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="SA_B_Sunny.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>
  </div>
</div>
</section>

<section class="hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle">
        360Loc dataset [3]
      </h2>
      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            Atrium Environment - Query Day / Database Day
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="atrium_daytime1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            Atrium Environment - Query Night / Database Day
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="atrium_nighttime2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            Concourse Environment - Query Day / Database Day
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="concourse_daytime0.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <h2 class="subtitle has-text-centered is-size-4">
            Hall Environment - Query Night / Database Day
          </h2>
          <video poster="" autoplay controls muted loop style="width: 100%">
            <source src="hall_nighttime2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Image carousel -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Highlighted Examples</h2>
      <h2 class="subtitle">
        COLD database [2]
      </h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/FR_A_Sunny.png" alt="fr_a_wrong" class="center" width="60%"/>
        <h2 class="subtitle has-text-centered">
          R@1% successful retrieval example in the FR-A environment under sunny conditions with our method.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/FR_B_Sunny_correct.png" alt="fr_b_correct" class="center" width="60%"/>
        <h2 class="subtitle has-text-centered">
          R@1 successful retrieval example in the FR-B environment under sunny conditions with our method.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/SA_A_Night_correct.png" alt="sa_a_correct" class="center" width="60%"/>
        <h2 class="subtitle has-text-centered">
          R@1 successful retrieval example in the SA-A environment under night conditions with our method.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/SA_B_Night.png" alt="sa_b_wrong" class="center" width="60%"/>
      <h2 class="subtitle has-text-centered">
        Wrong retrieval example in the SA-B environment under night conditions with our method.
      </h2>
    </div>
  </div>
</div>
</div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container">
      <h2 class="subtitle">
        360Loc database [3]
      </h2>
      <div id="results-carousel-2" class="carousel results-carousel">
    <div class="item">
        <img src="static/images/atrium_nighttime2_correct.png" alt="atrium_nighttime2_correct" class="center" width="60%"/>
        <h2 class="subtitle has-text-centered">
          R@1 successful retrieval example in the atrium environment under nighttime conditions with our method.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/concourse_daytime2.png" alt="concourse_daytime2" class="center" width="60%"/>
        <h2 class="subtitle has-text-centered">
          Wrong retrieval example in the concourse environment under daytime conditions with our method.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/hall_nighttime2_correct.png" alt="hall_nighttime2_correct" class="center" width="60%"/>
        <h2 class="subtitle has-text-centered">
          R@1 successful retrieval example in the hall environment under nighttime conditions with our method.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/piatrium_nighttime0.png" alt="piatrium_nighttime0" class="center" width="60%"/>
      <h2 class="subtitle has-text-centered">
        Wrong retrieval example in the piatrium environment under nighttime conditions with our method.
      </h2>
    </div>
  </div>
</div>
</section>
<!-- End image carousel -->


<!-- Añade una sección para cargar dos imágenes, una en una fila y otra después -->
<!-- Bibliography-->
<section class="hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">References</h2>
      <div class="content" style="margin-top: 2rem;">
        <ol>
          <li>Yang, L., Kang, B., Huang, Z., Zhao, Z., Xu, X., Feng, J., & Zhao, H. (2024). Depth anything v2. Advances in Neural Information Processing Systems, 37, 21875-21911.</li>
          <li>Pronobis, A., & Caputo, B. (2009). COLD: The CoSy localization database. The International Journal of Robotics Research, 28(5), 588-594.</li>
          <li>Huang, H., Liu, C., Zhu, Y., Cheng, H., Braud, T., & Yeung, S. K. (2024). 360loc: A dataset and benchmark for omnidirectional visual localization with cross-device queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22314-22324).</li>
        </ol>
      </div>
    </div>
  </div>
</section>
<!--End Bibliography-->


<!--BibTex citation-->

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      Soon available.
    </div>
</section>
<!--
<pre><code>@misc{alfaro2024visual,
        title={Visual Place Recognition Using Omnidirectional Cameras and Monocular Depth Estimation},
        author={M. Alfaro and J. J. Cabrera and A. Gil and O. Reinoso and L. Payá},
        year={2024},
        eprint={2403.14594},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
      }</code></pre>-->
<!--End BibTex citation-->



<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Comparison with other methods</h2>
      <div class="item">
        <img src="static/images/table.png" alt="new_image_1" class="center" width="150%"/>
        <h2 class="subtitle has-text-centered">
          Comparison in terms of Recall@1 (R@1) and Recall@1% (R@1%) with state-of-the-art methods.
        </h2>
      </div>
    
      <div class="content" style="margin-top: 2rem;"></div>
        <p style="text-align: justify;">
          <strong>Bibliography:</strong><br>
          [1] Uy, M. A., Pham, Q. H., Hua, B.-S., Nguyen, T., & Yeung, S.-K. (2018). PointNetVLAD: Deep point cloud based retrieval for large-scale place recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4470-4479).<br>
          [2] Zhang, J., Hua, B.-S., & Yeung, S.-K. (2019). PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 12436-12445).<br>
          [3] Sun, Y., & Chen, X. (2020). DAGC: Data-Augmentation and Graph Convolution Network for 3D Point Cloud Classification. IEEE Transactions on Multimedia, 22(9), 2237-2249.<br>
          [4] Liu, Z., Tang, H., Lin, Y., & Han, S. (2019). LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis. In Proceedings of the IEEE International Conference on Computer Vision (pp. 12308-12317).<br>
          [5] Xia, Y., Chen, X., & Zhang, H. (2021). SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition. IEEE Transactions on Multimedia, 23, 2751-2763.<br>
          [6] Komorowski, J. (2021). MinkLoc3D: Point Cloud Based Large-Scale Place Recognition. In Proceedings of the IEEE International Conference on 3D Vision (pp. 1080-1088).<br>
          [7] Hui, L., Yi, L., Wu, Z., Qi, C. R., & Guibas, L. J. (2021). PPT-Net: Point Pair Transformation Network for Efficient Point Cloud Registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1578-1587).<br>
          [8] Fan, L., Pang, J., Yang, Y., & Tian, Y. (2022). SVT-Net: Supervised Volumetric Transformer Network for Place Recognition Using 3D Point Clouds. IEEE Transactions on Neural Networks and Learning Systems.<br>
          [9] Xu, H., Zhang, J., & Yeung, S.-K. (2021). TransLoc3D: A Transformer Network for Place Recognition using 3D Point Clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3677-3686).<br>
          [10] Komorowski, J. (2022). Improving Large-Scale Place Recognition Using MinkLoc3D and Sparse Voxelization. IEEE Transactions on Image Processing, 31, 3054-3068.<br>
        </p>
      </div>
    </div>
  </div>
</section> -->





<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{cabrera2024minkunext,
        title={MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions},
        author={J. J. Cabrera and A. Santo and A. Gil and C. Viegas and L. Payá},
        year={2024},
        eprint={2403.07593},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
      }</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
